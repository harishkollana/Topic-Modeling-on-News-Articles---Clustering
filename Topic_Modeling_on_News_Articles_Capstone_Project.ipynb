{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishkollana/Topic-Modeling-on-News-Articles-Clustering/blob/main/Topic_Modeling_on_News_Articles_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOGC-qoyhJeX"
      },
      "source": [
        "# <b><u> Project Title : Extraction/identification of major topics & themes discussed in news articles. </u></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y06xIdG26kRF"
      },
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        "### In this project your task is to identify major themes/topics across a collection of BBC news articles. You can use clustering algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlLxAtlziMbP"
      },
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### The dataset contains a set of news articles for each major segment consisting of business, entertainment, politics, sports and technology. You need to create an aggregate dataset of all the news articles and perform topic modeling on this dataset. Verify whether these topics correspond to the different tags available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dByMsuzT8Tnw",
        "outputId": "e97ab75c-fa39-42e9-facf-c4db0b4d6946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#import libraries for topic modeling on news articles\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import glob\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raEhkLbQHZnP",
        "outputId": "be007165-6eeb-4cc0-d80b-50a53aa4c300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=ea0ca8fdec2de8315e1c423592e79e3ad0587671d861c54e9f36f79203c209d1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ufdrhrhp/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0YxdZ_mHZnQ",
        "outputId": "d2018453-58a6-4663-9b6d-ad6cab02729a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==3.2.1\n",
            "  Downloading pyLDAvis-3.2.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.1.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (2.11.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==3.2.1) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==3.2.1) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis==3.2.1) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis==3.2.1) (3.0.7)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.1-py2.py3-none-any.whl size=136187 sha256=cd1eb586102aad7abf53a8abc74655e64de7074574e52a9bc52d11ebab68f8f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/ee/a6/7c17a63623f940dff0b9cbd7e48a27543f088fa55a7d2b62d0\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyLDAvis==3.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P6WrHmeqHZnQ",
        "outputId": "2ee859c9-4b30-4894-b038-cf0c0ef04e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-profiling in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting pandas-profiling\n",
            "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting joblib~=1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.7-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.62.3)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 39.2 MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.3.5)\n",
            "Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.0.1)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.5.0)\n",
            "Collecting tangled-up-in-unicode==0.1.0\n",
            "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 46.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.21.5)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.4.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Collecting scipy>=1.4.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.0.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.2.0)\n",
            "Building wheels for collected packages: htmlmin, imagehash\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=a80df23c892400cc9756e456813c16a6320ef772f279bc46fe23f0ae8a2ff67c\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=ca56fcef8eba41b196b689ee9e9fe77eb019d2c786d51d4caa26f5b1b84ffee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built htmlmin imagehash\n",
            "Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, joblib, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 joblib-1.0.1 multimethod-1.7 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.9.0 requests-2.27.1 scipy-1.7.3 tangled-up-in-unicode-0.1.0 visions-0.7.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U pandas-profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRa5d_mcHZnR"
      },
      "source": [
        "Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvGqvtc_IRpT",
        "outputId": "daad8d6f-0b8b-47be-d40b-c790ccccb201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Daw5ochTHZnS"
      },
      "outputs": [],
      "source": [
        "#load path to data\n",
        "path = '/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BbvmeT29K8Y3"
      },
      "outputs": [],
      "source": [
        "#Importing text file paths\n",
        "business = glob.glob(path+'/business/*')\n",
        "entertainment = glob.glob(path+'/entertainment/*')\n",
        "politics = glob.glob(path+'/politics/*')\n",
        "sports = glob.glob(path+'/sport/*')\n",
        "tech = glob.glob(path+'/tech/*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Y12DH6K_Hi",
        "outputId": "de91f18f-acce-476b-956a-4a7aa4641481"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc/business/507.txt',\n",
              " '/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc/business/505.txt',\n",
              " '/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc/business/471.txt',\n",
              " '/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc/business/498.txt',\n",
              " '/content/drive/MyDrive/Capstone Projects/Topic Modeling on News Articles/bbc/business/510.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "business[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TL-4cXbVLGLW"
      },
      "outputs": [],
      "source": [
        "# Making the data lists for different topics.\n",
        "def make_list(data):\n",
        "    list = []\n",
        "    for i in range(len(data)):\n",
        "      file = open(data[i],'r')\n",
        "      list.append(file.read())\n",
        "    return(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "camH5f9xLpCw",
        "outputId": "684b8082-d17c-49e0-e645-1f6e7b1ba90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n",
            "List ended !!\n"
          ]
        }
      ],
      "source": [
        "sports_text=[]\n",
        "\n",
        "for i in range(len(sports)):\n",
        "  f=open(sports[i],errors='ignore')\n",
        "  a=f.read()\n",
        "  sports_text.append(a)\n",
        "\n",
        "  print('List ended !!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "FYL3ULR9L510",
        "outputId": "2923532a-fa7e-4658-9bb8-27024c239bee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Henman to face Saulnier test\\n\\nBritish number one Tim Henman will face France\\'s Cyril Saulnier in the first round of next week\\'s Australian Open.\\n\\nGreg Rusedski, the British number two, is in the same quarter of the draw and could face Andy Roddick in the second round if he beats Swede Jonas Bjorkman. Local favourite Lleyton Hewitt will meet France\\'s Arnaud Clement, while defending champion and world number one Roger Federer faces Fabrice Santoro. Women\\'s top seed Lindsay Davenport drew Spanish veteran Conchita Martinez.\\n\\nHenman came from two sets down to defeat Saulnier in the first round of the French Open last year, so he knows he faces a tough test in Melbourne. The seventh seed, who has never gone beyond the quarter-finals in the year\\'s first major and is lined up to meet Roddick in the last eight, is looking forward to the match. \"He\\'s tough player on any surface, he\\'s got a lot of ability,\" he said. \"We had a really tight one in Paris that went my way so I\\'m going to need to play well from the outset because he\\'s a dangerous competitor.\" Switzerland\\'s Federer, seeded one, is the hot favourite having won three of the four grand slam titles in 2004. He has beaten Santoro in five of their seven previous encounters, but is taking nothing for granted. \"It\\'s a tricky match,\" Federer said. \"I played him at the US Open and won quite comfortably then. But you never know, if the rhythm is a bit off, he can keep you guessing and make it difficult. \"The most important thing, though, is to get used to playing five-set matches and winning them.\" The 23-year-old could meet four-time champion Andre Agassi in the quarter-finals before meeting Russian Marat Safin, the player he beat in last year\\'s final.\\n\\nEighth-seeded American Agassi is set to play a qualifier in round one if he can shake off a hip injury which ruled him out of the Kooyong Classic. Second seed Andy Roddick will open his campaign against Irakli Labadze of Georgia. The American could meet Rusedski in the second round, seventh seed Henman in the quarter-finals and Hewitt in the last four. Hewitt is hoping to become the first Australian man to win the event since Mark Edmondson in 1976. The 23-year-old has never been beyond round four in eight attempts at Melbourne Park but has at least secured the opposite half of the draw to Federer, who beat him in the Australian Open, Wimbledon and US Open last year. Safin, seeded four, opens his campaign against a qualifier with 16th seed Tommy Haas, the player he beat in the semi-finals in 2002, a possible fourth-round opponent.\\n\\nIn the women\\'s draw, Davenport could encounter eighth-seeded Venus Williams in the quarter-finals and third-ranked Anastasia Myskina, the French Open champion, in the semi-finals. Bronchitis ruled Davenport, the 2000 Australian Open champion, out of her Sydney quarter-final on Thursday. Venus Williams, who lost to younger sister Serena in the Melbourne final two years ago, opens against Eleni Daniilidou of Greece. Serena Williams, who won her fourth consecutive grand slam at the 2003 Australian Open, was drawn in the bottom quarter with second seed Amelie Mauresmo, a runner-up in 1999. Serena will open against another Frenchwoman Camille Pin, while Mauresmo plays Australia\\'s Samantha Stosur. Wimbledon champion Maria Sharapova, seeded fourth, drew a qualifier in the first round but could meet fellow Russian Svetlana Kuznetsova, the US Open winner, in the last eight\\n\\n1 Roger Federer (Switzerland)\\n\\n2 Andy Roddick (US)\\n\\n3 Lleyton Hewitt (Australia)\\n\\n4 Marat Safin (Russia)\\n\\n5 Carlos Moya (Spain)\\n\\n6 Guillermo Coria (Argentina)\\n\\n7 Tim Henman (Britain)\\n\\n8 Andre Agassi (US)\\n\\n9 David Nalbandian (Argentina)\\n\\n10 Gaston Gaudio (Argentina)\\n\\n11 Joachim Johansson (Sweden)\\n\\n12 Guillermo Canas (Argentina)\\n\\n13 Tommy Robredo (Spain)\\n\\n14 Sebastien Grosjean (France)\\n\\n15 Mikhail Youzhny (Russia)\\n\\n16 Tommy Haas (Germany)\\n\\n17 Andrei Pavel (Romania)\\n\\n18 Nicolas Massu (Chile)\\n\\n19 Vincent Spadea (US)\\n\\n20 Dominik Hrbaty (Slovakia)\\n\\n21 Nicolas Kiefer (Germany)\\n\\n22 Ivan Ljubicic (Croatia)\\n\\n23 Fernando Gonzalez (Chile)\\n\\n24 Feliciano Lopez (Spain)\\n\\n25 Juan Ignacio Chela (Argentina)\\n\\n26 Nikolay Davydenko (Russia)\\n\\n27 Paradorn Srichaphan (Thailand)\\n\\n28 Mario Ancic (Croatia)\\n\\n29 Taylor Dent (US)\\n\\n30 Thomas Johansson (Sweden)\\n\\n31 Juan Carlos Ferrero (Spain)\\n\\n32 Jurgen Melzer (Austria)\\n\\n1 Lindsay Davenport (US)\\n\\n2 Amelie Mauresmo (France)\\n\\n3 Anastasia Myskina (Russia)\\n\\n4 Maria Sharapova (Russia)\\n\\n5 Svetlana Kuznetsova (Russia)\\n\\n6 Elena Dementieva (Russia)\\n\\n7 Serena Williams (US)\\n\\n8 Venus Williams (US)\\n\\n9 Vera Zvonareva (Russia)\\n\\n10 Alicia Molik (Australia)\\n\\n11 Nadia Petrova (Russia)\\n\\n12 Patty Schnyder (Switzerland)\\n\\n13 Karolina Sprem (Croatia)\\n\\n14 Francesca Schiavone (Italy)\\n\\n15 Silvia Farina Elia (Italy)\\n\\n16 Ai Sugiyama (Japan)\\n\\n17 Fabiola Zuluaga (Colombia)\\n\\n18 Elena Likhovtseva (Russia)\\n\\n19 Nathalie Dechy (France)\\n\\n20 Tatiana Golovin (France)\\n\\n21 Amy Frazier (US)\\n\\n22 Magdalena Maleeva (Bulgaria)\\n\\n23 Jelena Jankovic (Serbia and Montenegro)\\n\\n24 Mary Pierce (France)\\n\\n25 Lisa Raymond (US)\\n\\n26 Daniela Hantuchova (Slovakia)\\n\\n27 Anna Smashnova (Israel)\\n\\n28 Shinobu Asagoe (Japan)\\n\\n29 Gisela Dulko (Argentina)\\n\\n30 Flavia Pennetta (Italy)\\n\\n31 Jelena Kostanic (Croatia)\\n\\n32 Iveta Benesova (Czech Republic)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sports_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1qdz8Y1LPg8"
      },
      "outputs": [],
      "source": [
        "business_texts= make_list(business)\n",
        "entertainment_text = make_list(entertainment)\n",
        "politics_texts= make_list(politics)\n",
        "tech_text = make_list(tech)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2eeF4EnLSz8"
      },
      "outputs": [],
      "source": [
        "#Number of documents in every topics\n",
        "print(len(business_texts),len(entertainment_text),len(politics_texts),len(sports_text),len(tech_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5eREl6UMC-T"
      },
      "outputs": [],
      "source": [
        "# Combine the topics.\n",
        "complete_text = business_texts + entertainment_text + politics_texts + sports_text + tech_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TNexxrbMFg7"
      },
      "outputs": [],
      "source": [
        "len(complete_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13f4QGW6MIdR"
      },
      "source": [
        "From the above we can see that, the length of the complete text is 2225."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZOxUhlPMNCM"
      },
      "outputs": [],
      "source": [
        "# Make the dataframe of texts.\n",
        "df = pd.DataFrame({'text': complete_text, 'type': ['business']*len(business_texts) + ['entertainment']*len(entertainment_text) + ['politics']*len(politics_texts) + ['sport']*len(sports_text) + ['tech']*len(tech_text)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReeEQJS7HZnV"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SKQgF3rJ1BP"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxfMZPfAJ4u4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FJleHkTJ6wO"
      },
      "outputs": [],
      "source": [
        "# removing duplicate observations\n",
        "df=df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3HtLpL1HZnV"
      },
      "outputs": [],
      "source": [
        "# Removal of \"\\n\"\n",
        "# Converting the words to the lowercase.\n",
        "# Removal of stopword.\n",
        "\n",
        "def text_processing(data):\n",
        "  data = data.map(lambda x: x.replace('\\n',' '))\n",
        "  data = data.map(lambda x: x.lower())\n",
        "  #data = data.map(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
        "  data = data.map(lambda x: ' '.join([i for i in x.split(' ') if i not in stopwords.words('english')]))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asuAlM-eJa6j"
      },
      "outputs": [],
      "source": [
        "# Converting column into astring\n",
        "df['text'] = df['text'].astype('str') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbUFq0bzHZnV"
      },
      "outputs": [],
      "source": [
        "#check data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQZTNZ8vHZnW"
      },
      "outputs": [],
      "source": [
        "#add a new column for number of sentences in the text\n",
        "df['sentence_count'] = [len(i) for i in df['text'].apply(nltk.sent_tokenize)]\n",
        "\n",
        "#remove punctuation\n",
        "df['text'] = df['text'].map(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
        "\n",
        "#add a new column for number of words in the text\n",
        "df['word_count'] = [len(i.split()) for i in df['text']]\n",
        "\n",
        "#apply lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "#add a new column for number of characters in the text\n",
        "df['char_count'] = df['text'].str.len()\n",
        "\n",
        "#add a new column for average sentence length in the text\n",
        "df['avg_sentence_length'] = df['word_count']/df['sentence_count']\n",
        "\n",
        "#add a new column for average word length in the text\n",
        "df['avg_word_length'] = df['char_count']/df['word_count']\n",
        "\n",
        "#add a new column for number of unique words in the text\n",
        "df['unique_word_count'] = df['text'].apply(lambda x: len(set(x.split(' '))))\n",
        "\n",
        "#add a new column for number of digits in the text\n",
        "df['digit_count'] = df['text'].apply(lambda x: len([c for c in x if c in string.digits]))\n",
        "\n",
        "#check data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t07Tsw_fHZnW"
      },
      "source": [
        "Explorative Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29ZYc1jPHZnW"
      },
      "outputs": [],
      "source": [
        "#create a word cloud for popular words in each topic\n",
        "def word_cloud(data, topic):\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as mpatches\n",
        "    import matplotlib.lines as mlines\n",
        "    from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "    from PIL import Image\n",
        "    import nltk\n",
        "    nltk.download('wordnet')\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from wordcloud import WordCloud\n",
        "    from nltk.corpus import stopwords\n",
        "    stop = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    wordcloud = WordCloud(background_color=\"white\", max_words=100, stopwords=stop, max_font_size=40,\n",
        "                          scale=3, random_state=1).generate(' '.join([i for i in data['text'] if i not in stop]))\n",
        "    fig = plt.figure(1, figsize=(20, 20))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.title(topic)\n",
        "    plt.show()\n",
        "\n",
        "word_cloud(df, 'business')\n",
        "word_cloud(df, 'entertainment')\n",
        "word_cloud(df, 'politics')\n",
        "word_cloud(df, 'sport')\n",
        "word_cloud(df, 'tech')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fJu0SpnHZnX"
      },
      "outputs": [],
      "source": [
        "#create a word cloud for popular words in all topics\n",
        "def word_cloud_all(data):\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as mpatches\n",
        "    import matplotlib.lines as mlines\n",
        "    from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "    from PIL import Image\n",
        "    import nltk\n",
        "    nltk.download('wordnet')\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from wordcloud import WordCloud\n",
        "    from nltk.corpus import stopwords\n",
        "    stop = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    wordcloud = WordCloud(background_color=\"white\", max_words=100, stopwords=stop, max_font_size=40,\n",
        "                          scale=3, random_state=1).generate(' '.join([i for i in data['text'] if i not in stop]))\n",
        "    fig = plt.figure(1, figsize=(20, 20))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.title('All topics')\n",
        "    plt.show()\n",
        "\n",
        "word_cloud_all(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdEKjvD7HZnX"
      },
      "outputs": [],
      "source": [
        "#set figure size\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "\n",
        "#create a countplot for type in df\n",
        "sns.countplot(x='type', data=df)\n",
        "plt.xlabel(\"Type Of News\")\n",
        "plt.ylabel(\"News Count\")\n",
        "plt.title(\"Type Of news Counts\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv7qFTUuHZnX"
      },
      "outputs": [],
      "source": [
        "#create pairplot for all the features\n",
        "sns.pairplot(df, hue='type', palette='Set1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV1M5b8UHZnY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ9_m8_uHZnY"
      },
      "source": [
        "Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSBoM_5EHZnY"
      },
      "source": [
        "hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B4o-Xz8HZnY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLt8YvS5HZnZ"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(decode_error='ignore', lowercase = True, min_df=2)\n",
        "\n",
        "dtm=tfidf.fit_transform(df['text'])\n",
        "\n",
        "dtm.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9xJqiXoHZnZ"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LaPYEMiHZnZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "X = dtm.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzR38bVNHZnZ"
      },
      "outputs": [],
      "source": [
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(X)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode='level', p=3)\n",
        "#plt.xlabel(\"Number of points in node\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66dbaCV1HZnZ"
      },
      "source": [
        "From the above dendogram we can see that, we have successfully got 5 different clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN6WeQUeHZna"
      },
      "outputs": [],
      "source": [
        "clustering = AgglomerativeClustering(n_clusters=5).fit(X)\n",
        "clustering\n",
        "AgglomerativeClustering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wi9V_QMHZna"
      },
      "outputs": [],
      "source": [
        "pd.Series(clustering.labels_).unique()\n",
        "\n",
        "heirdf=pd.DataFrame(dtm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrxc5bk4HZna"
      },
      "outputs": [],
      "source": [
        "heirdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmCSw3ggHZna"
      },
      "source": [
        "Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwN_RWFwQ3LN"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soiA1GTpQvAE"
      },
      "outputs": [],
      "source": [
        "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTThwbosHZna"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYvSeaqJHZnb"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the words.\n",
        "\n",
        "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n",
        "       output = []\n",
        "       for sent in texts:\n",
        "             doc = nlp(sent) \n",
        "             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
        "       return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAUPso7BHZnb"
      },
      "outputs": [],
      "source": [
        "text_list=df['text'].tolist()\n",
        "print(text_list[1])\n",
        "tokenized_texts = lemmatization(text_list)\n",
        "print(tokenized_texts[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NS7usS9HZnb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Topic Modeling on News Articles - Capstone Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}